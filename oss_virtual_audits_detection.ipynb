{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52b221b0-1382-4128-b0c2-33ce5ce01441",
   "metadata": {},
   "source": [
    "# Object detection and segmentation with the Gemini API\n",
    "\n",
    "\n",
    "**References:**\n",
    "* [Conversational image segmentation with Gemini 2.5](https://developers.googleblog.com/en/conversational-image-segmentation-gemini-2-5/)\n",
    "* [Use Gemini 2.5 for Zero-Shot Object Detection & Segmentation](https://blog.roboflow.com/gemini-2-5-object-detection-segmentation/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9a6f0f-3165-4984-97b7-8a8d628cfac2",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f032f4-11f1-4c19-9a84-b5155fe573bb",
   "metadata": {},
   "source": [
    "### Software Development Kit (SDK)\n",
    "\n",
    "Installing and initializing the Google Generative AI SDK, a collection of tools and libraries that allow developers to interact with Google's Gemini models.  \n",
    "This setup enables performing tasks such as text generation, image analysis, and segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e932a5-6fe9-4f2f-9b83-989f0461de67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install\n",
    "#!pip install google-genai supervision\n",
    "\n",
    "# Initialize\n",
    "from google import genai\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52846477-37a8-4128-a294-666f4f309a84",
   "metadata": {},
   "source": [
    "### Application Programming Interface (API) key and SDK client\n",
    "\n",
    "The API key is loaded from an environment file (.env) to keep credentials secure.  \n",
    "The client object authenticates requests and enables communication with Gemini models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12f2b22-15a3-4e64-9722-f8ef6e81bde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key from environment file\n",
    "load_dotenv(\"gemini_api_key.env\")\n",
    "client = genai.Client(api_key = os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291b038f-b195-4de4-9794-785c1c9c7b0a",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Spatial understanding works best with the **Gemini 2.0 Flash** model.  \n",
    "It performs even better with **Gemini 2.5 models** (like `gemini-2.5-pro`), which are more capable \"thinking models\" — though slightly slower.  \n",
    "Some advanced features, such as **image segmentation**, are only supported by **Gemini 2.5 models**.\n",
    "\n",
    "**Available model options:**\n",
    "- `gemini-2.0-flash`\n",
    "- `gemini-2.5-flash-lite`\n",
    "- `gemini-2.5-flash-lite-preview-09-2025`\n",
    "- `gemini-2.5-flash`\n",
    "- `gemini-2.5-flash-preview-09-2025`\n",
    "- `gemini-2.5-pro`\n",
    "\n",
    "**Temperature (creativity control):**\n",
    "- Controls randomness/creativity of model output.  \n",
    "- `0.0` → very deterministic, consistent output (best for precise tasks).  \n",
    "- `0.5` → moderately creative (good balance for tasks like bounding boxes).  \n",
    "- `1.0` → more creative, potentially less consistent output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b064fe74-99fd-4947-a29f-342f3fa24765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "model_gemini_2_0 = \"gemini-2.0-flash\" \n",
    "model_gemini_2_5 = \"gemini-2.5-flash\"\n",
    "\n",
    "# Temperature\n",
    "temperature = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fee2b1f-2bd4-4124-b0aa-2c9c83eeb58d",
   "metadata": {},
   "source": [
    "### Safety settings\n",
    "\n",
    "This parameter specifies how the model should handle potentially harmful content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843cd4ec-98cc-45e3-aa75-2c838b60df14",
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_settings = [types.SafetySetting(category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                                       threshold=\"BLOCK_ONLY_HIGH\",),]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d25cc1d-67b6-494d-9141-9d15a4d43cbe",
   "metadata": {},
   "source": [
    "### Other packages\n",
    "\n",
    "Installing and importing additional packages required for image processing and data handling.  \n",
    "- **Pillow (PIL)** is used to open and manipulate images.  \n",
    "- **io** and **requests** modules help handle image data streams and HTTP requests.  \n",
    "- **Supervision (sv)** is used for utilities like plotting bounding boxes and working with detection results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0366e0-89b9-41cb-b120-d0ace24ba9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "\n",
    "#!pip install Pillow\n",
    "from PIL import Image\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import io\n",
    "from io import BytesIO\n",
    "\n",
    "import requests\n",
    "\n",
    "import supervision as sv\n",
    "\n",
    "import json\n",
    "import base64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c65336c-a541-4036-8f82-357b10bdcd22",
   "metadata": {},
   "source": [
    "### Error handling and retry parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93da0ee9-d90f-4fc9-93d0-707c53df5172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import time\n",
    "from google.genai import errors as genai_errors\n",
    "\n",
    "# Define retry configuration\n",
    "max_retries = 10\n",
    "wait_time = 1\n",
    "current_retry = 0\n",
    "response = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75e9a86-0ea0-46ba-b856-ad1a977b5356",
   "metadata": {},
   "source": [
    "## Object detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872188f4-f895-451d-b05e-420a4487829e",
   "metadata": {},
   "source": [
    "### Dog 1.jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42abc997-d2a1-4b0c-bb6a-67fe0c9c080a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root folder\n",
    "root_path = Path(r\"C:\\Users\\amand\\Amanda\\GitHub\\virtual_audit_ai\\street_view_images\")\n",
    "\n",
    "# Get all images recursively\n",
    "all_images = list(root_path.rglob(\"*.[jp][pn]g\"))  # matches .jpg, .jpeg, .png\n",
    "\n",
    "print(f\"Total images found: {len(all_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf889ff-be3b-44f2-8928-20db54f6fd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Prompt for Street View object detection\n",
    "# -----------------------------\n",
    "prompt = (\n",
    "    \"You are performing object detection on a Street View image. \"\n",
    "    \"Identify all visible objects in the scene (e.g., cars, pedestrians, traffic signs, street lights, trees, buildings, sidewalks, bicycles, etc.). \"\n",
    "    \"Output a JSON list of bounding boxes. Each entry should contain:\\n\"\n",
    "    \" - 'box_2d': the coordinates of the 2D bounding box\\n\"\n",
    "    \" - 'label': a descriptive text label of the object\\n\"\n",
    "    \"Use clear, descriptive labels for each object.\"\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Output folder\n",
    "# -----------------------------\n",
    "output_folder = Path(r\"C:\\Users\\amand\\Amanda\\GitHub\\virtual_audit_ai\\street_view_outputs\")\n",
    "output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Process all images\n",
    "# -----------------------------\n",
    "for image_path in all_images:\n",
    "    print(f\"\\nProcessing image: {image_path}\")\n",
    "\n",
    "    # Load and resize\n",
    "    image = Image.open(image_path)\n",
    "    width, height = image.size\n",
    "    target_height = int(1024 * height / width)\n",
    "    resized_image = image.resize((1024, target_height), Image.Resampling.LANCZOS)\n",
    "\n",
    "    # Reset retry variables\n",
    "    current_retry = 0\n",
    "    wait_time = 1\n",
    "    detections_json = None\n",
    "\n",
    "    # -----------------------------\n",
    "    # Retry loop for API call with JSON validation\n",
    "    # -----------------------------\n",
    "    while current_retry < max_retries:\n",
    "        try:\n",
    "            print(f\"Attempt {current_retry + 1}/{max_retries} to call the API...\")\n",
    "\n",
    "            response = client.models.generate_content(\n",
    "                model=model_gemini_2_0,\n",
    "                contents=[resized_image, prompt],\n",
    "                config=types.GenerateContentConfig(\n",
    "                    temperature=temperature,\n",
    "                    safety_settings=safety_settings,\n",
    "                    thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Extract JSON from response text\n",
    "            text = response.text\n",
    "            json_start = text.find(\"[\")\n",
    "            json_end = text.rfind(\"]\") + 1\n",
    "            try:\n",
    "                detections_json = json.loads(text[json_start:json_end])\n",
    "                print(f\"Valid JSON detected, proceeding with {len(detections_json)} objects.\")\n",
    "                break  # JSON valid -> exit retry loop\n",
    "            except Exception:\n",
    "                current_retry += 1\n",
    "                print(f\"Invalid JSON in response. Retrying {current_retry}/{max_retries}...\")\n",
    "                time.sleep(wait_time)\n",
    "                wait_time *= 2\n",
    "                continue\n",
    "\n",
    "        except genai_errors.ClientError as e:\n",
    "            current_retry += 1\n",
    "            print(f\"ClientError, retrying {current_retry}/{max_retries}... Error: {e}\")\n",
    "            time.sleep(wait_time)\n",
    "            wait_time *= 2\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "            break\n",
    "\n",
    "    # Skip image if no valid JSON after retries\n",
    "    if not detections_json:\n",
    "        print(f\"Failed to get valid detections for {image_path.name}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # -----------------------------\n",
    "    # Convert to Supervision Detections\n",
    "    # -----------------------------\n",
    "    resolution_wh = resized_image.size\n",
    "    detections = sv.Detections.from_vlm(\n",
    "        vlm=sv.VLM.GOOGLE_GEMINI_2_5,\n",
    "        result=json.dumps(detections_json),\n",
    "        resolution_wh=resolution_wh\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # Annotate image\n",
    "    # -----------------------------\n",
    "    thickness = sv.calculate_optimal_line_thickness(resolution_wh)\n",
    "    text_scale = sv.calculate_optimal_text_scale(resolution_wh)\n",
    "\n",
    "    box_annotator = sv.BoxAnnotator(thickness=thickness)\n",
    "    label_annotator = sv.LabelAnnotator(\n",
    "        smart_position=True,\n",
    "        text_color=sv.Color.BLACK,\n",
    "        text_scale=text_scale,\n",
    "        text_position=sv.Position.CENTER\n",
    "    )\n",
    "\n",
    "    annotated = resized_image\n",
    "    for annotator in (box_annotator, label_annotator):\n",
    "        annotated = annotator.annotate(scene=annotated, detections=detections)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Save annotated image\n",
    "    # -----------------------------\n",
    "    output_path = output_folder / f\"{image_path.stem}_annotated.jpg\"\n",
    "    annotated.save(output_path)\n",
    "    print(f\"Saved annotated image: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fcd217-0390-4808-b2bb-427f0cc36358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
